{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00004eb1-49e9-43b0-a239-1cdbc9772121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\saart\\OneDrive\\Desktop\\UAS_DTU_Round_2_Task_data\\4\\Front4.jpg: 480x640 1 2, 1 3, 575.2ms\n",
      "Speed: 8.0ms preprocess, 575.2ms inference, 7.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict16\u001b[0m\n",
      "\n",
      "image 1/1 C:\\Users\\saart\\OneDrive\\Desktop\\UAS_DTU_Round_2_Task_data\\4\\Back4.jpg: 480x640 1 3, 561.0ms\n",
      "Speed: 5.5ms preprocess, 561.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict16\u001b[0m\n",
      "[{'bbox': (360, 202, 462, 291), 'center': (411, 246), 'class': '3'}, {'bbox': (404, 240, 421, 271), 'center': (412, 255), 'class': '2'}]\n",
      "Final counts:\n",
      "red: 1\n",
      "green: 2\n",
      "blue: 0\n",
      "unknown: 0\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "front_image = r'C:\\Users\\saart\\OneDrive\\Desktop\\UAS_DTU_Round_2_Task_data\\4\\Front4.jpg'\n",
    "back_image = r'C:\\Users\\saart\\OneDrive\\Desktop\\UAS_DTU_Round_2_Task_data\\4\\Back4.jpg'\n",
    "model_path = r'C:\\Users\\saart\\OneDrive\\Desktop\\best.pt'\n",
    "model = YOLO(model_path)\n",
    "\n",
    "def predict(image_path):\n",
    "    results = model.predict(image_path, save=True)\n",
    "    fruits = []\n",
    "    for box in results[0].boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "        class_name = model.names[int(box.cls)]        \n",
    "        # Calculate center point\n",
    "        center_x = (x1 + x2) // 2\n",
    "        center_y = (y1 + y2) // 2\n",
    "        \n",
    "        fruits.append({\n",
    "            \"bbox\": (x1, y1, x2, y2),\n",
    "            \"center\": (center_x, center_y),\n",
    "            \"class\": class_name\n",
    "        })\n",
    "    \n",
    "    return fruits\n",
    "\n",
    "# Step 3: Simple color detection (RGB-based)\n",
    "def get_color(roi):\n",
    "    avg_color = np.mean(roi, axis=(0, 1))  # Get average RGB values\n",
    "    r, g, b = avg_color\n",
    "    \n",
    "    # Simple color thresholds (adjust these as needed)\n",
    "    if r > g and r > b: return \"red\"\n",
    "    elif g > r and g > b: return \"green\"\n",
    "    elif b > r and b > g: return \"blue\"\n",
    "    else: return \"unknown\"\n",
    "\n",
    "# Step 4: Process images\n",
    "front_img = cv2.imread(front_image)\n",
    "back_img = cv2.imread(back_image)\n",
    "\n",
    "front_fruits = predict(front_image)\n",
    "back_fruits = predict(back_image)\n",
    "\n",
    "# Step 5: Match fruits between front and back\n",
    "counts = {\"red\": 0, \"green\": 0, \"blue\": 0, \"unknown\": 0}\n",
    "\n",
    "# Check front fruits\n",
    "for front in front_fruits:\n",
    "    x1, y1, x2, y2 = front[\"bbox\"]\n",
    "    roi = front_img[y1:y2, x1:x2]\n",
    "    color = get_color(roi)\n",
    "    \n",
    "    # Check if same fruit exists in back image\n",
    "    is_duplicate = False\n",
    "    for back in back_fruits:\n",
    "        # Simple distance check (adjust 50 as needed)\n",
    "        distance = np.sqrt((front[\"center\"][0]-back[\"center\"][0])**2 + \n",
    "                          (front[\"center\"][1]-back[\"center\"][1])**2)\n",
    "        \n",
    "        if distance < 50:  # If centers are close\n",
    "            is_duplicate = True\n",
    "            break\n",
    "    \n",
    "    if not is_duplicate:\n",
    "        counts[color] += 1\n",
    "\n",
    "# Check remaining back fruits\n",
    "for back in back_fruits:\n",
    "    x1, y1, x2, y2 = back[\"bbox\"]\n",
    "    roi = back_img[y1:y2, x1:x2]\n",
    "    color = get_color(roi)\n",
    "    counts[color] += 1\n",
    "\n",
    "# Step 6: Show results\n",
    "print(front_fruits)\n",
    "print(\"Final counts:\")\n",
    "for color, count in counts.items():\n",
    "    print(f\"{color}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80eb5770-1fa9-45be-a2fb-64d8ec9ac6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs directory zipped as runs.zip\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='runs.zip' target='_blank'>runs.zip</a><br>"
      ],
      "text/plain": [
       "C:\\Users\\saart\\runs.zip"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Zip the entire runs directory\n",
    "shutil.make_archive('runs', 'zip', 'runs')\n",
    "print(\"Runs directory zipped as runs.zip\")\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Create a download link for the zipped file\n",
    "FileLink('runs.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d1dea-67ac-49d4-8d6e-da69b22a026e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e93a24f5-ac48-454f-a564-37daaf7881fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\saart\\OneDrive\\Desktop\\UAS_DTU_Round_2_Task_data\\4\\Front4.jpg: 480x640 1 2, 1 3, 675.1ms\n",
      "Speed: 10.6ms preprocess, 675.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\saart\\OneDrive\\Desktop\\UAS_DTU_Round_2_Task_data\\4\\Back4.jpg: 480x640 1 3, 632.3ms\n",
      "Speed: 7.7ms preprocess, 632.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Final Fruit Counts:\n",
      "Red: 0\n",
      "Green: 2\n",
      "Blue: 0\n",
      "Purple: 0\n",
      "Unknown: 0\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Paths configuration\n",
    "front_image = r'C:\\Users\\saart\\OneDrive\\Desktop\\UAS_DTU_Round_2_Task_data\\4\\Front4.jpg'\n",
    "back_image = r'C:\\Users\\saart\\OneDrive\\Desktop\\UAS_DTU_Round_2_Task_data\\4\\Back4.jpg'\n",
    "model_path = r'C:\\Users\\saart\\OneDrive\\Desktop\\best.pt'\n",
    "\n",
    "# Initialize YOLO model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "def predict(image_path):\n",
    "    \"\"\"Detect fruits using YOLO and return their information\"\"\"\n",
    "    results = model.predict(image_path, conf=0.5)  # Increased confidence threshold\n",
    "    fruits = []\n",
    "    \n",
    "    for box in results[0].boxes:\n",
    "        if box.conf < 0.5:  # Skip low-confidence detections\n",
    "            continue\n",
    "            \n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "        center_x = (x1 + x2) // 2\n",
    "        center_y = (y1 + y2) // 2\n",
    "        \n",
    "        fruits.append({\n",
    "            \"bbox\": (x1, y1, x2, y2),\n",
    "            \"center\": (center_x, center_y)\n",
    "        })\n",
    "    \n",
    "    return fruits\n",
    "\n",
    "def get_color(roi):\n",
    "    \"\"\"Detect color using HSV color space\"\"\"\n",
    "    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "    avg_hue = np.mean(hsv[:, :, 0])\n",
    "    \n",
    "    # Define HSV color ranges (Hue, Saturation, Value)\n",
    "    if 0 <= avg_hue < 15 or 160 <= avg_hue <= 180:\n",
    "        return \"red\"\n",
    "    elif 35 <= avg_hue < 85:\n",
    "        return \"green\"\n",
    "    elif 85 <= avg_hue < 100:\n",
    "        return \"blue\"\n",
    "    elif 100 <= avg_hue < 160:\n",
    "        return \"purple\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def align_images(front_img, back_img):\n",
    "    \"\"\"Align images using feature matching and return homography matrix\"\"\"\n",
    "    orb = cv2.ORB_create()\n",
    "    kp1, des1 = orb.detectAndCompute(front_img, None)\n",
    "    kp2, des2 = orb.detectAndCompute(back_img, None)\n",
    "    \n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(des1, des2)\n",
    "    \n",
    "    src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1,1,2)\n",
    "    \n",
    "    H, _ = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC, 5.0)\n",
    "    return H\n",
    "\n",
    "# Load images\n",
    "front_img = cv2.imread(front_image)\n",
    "back_img = cv2.imread(back_image)\n",
    "\n",
    "# Get fruit detections\n",
    "front_fruits = predict(front_image)\n",
    "back_fruits = predict(back_image)\n",
    "\n",
    "# Align images using homography\n",
    "H = align_images(front_img, back_img)\n",
    "\n",
    "# Initialize counters\n",
    "color_counts = {\"red\": 0, \"green\": 0, \"blue\": 0, \"purple\": 0, \"unknown\": 0}\n",
    "matched_pairs = set()\n",
    "\n",
    "# Process front image fruits\n",
    "for idx, front in enumerate(front_fruits):\n",
    "    x1, y1, x2, y2 = front[\"bbox\"]\n",
    "    roi = front_img[y1:y2, x1:x2]\n",
    "    color = get_color(roi)\n",
    "    \n",
    "    # Check for matches in back image\n",
    "    for back_idx, back in enumerate(back_fruits):\n",
    "        # Transform back fruit center to front image coordinates\n",
    "        back_center = np.array([[back[\"center\"]]], dtype=np.float32)\n",
    "        transformed_center = cv2.perspectiveTransform(back_center, H)[0][0]\n",
    "        \n",
    "        # Calculate distance in aligned coordinates\n",
    "        distance = np.linalg.norm(np.array(front[\"center\"]) - transformed_center)\n",
    "        \n",
    "        if distance < 30:  # Match threshold in pixels\n",
    "            matched_pairs.add(back_idx)\n",
    "            # Take color from both views for verification\n",
    "            back_roi = back_img[back[\"bbox\"][1]:back[\"bbox\"][3], back[\"bbox\"][0]:back[\"bbox\"][2]]\n",
    "            back_color = get_color(back_roi)\n",
    "            \n",
    "            if color == back_color:\n",
    "                color_counts[color] += 1\n",
    "            break\n",
    "    else:\n",
    "        color_counts[color] += 1\n",
    "\n",
    "# Process unmatched back fruits\n",
    "for idx, back in enumerate(back_fruits):\n",
    "    if idx not in matched_pairs:\n",
    "        roi = back_img[back[\"bbox\"][1]:back[\"bbox\"][3], back[\"bbox\"][0]:back[\"bbox\"][2]]\n",
    "        color = get_color(roi)\n",
    "        color_counts[color] += 1\n",
    "\n",
    "# Display results\n",
    "print(\"Final Fruit Counts:\")\n",
    "for color, count in color_counts.items():\n",
    "    print(f\"{color.capitalize()}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15b4a1c8-06e7-4513-aa9d-67e1520c7de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\saart\\OneDrive\\Desktop\\UAS_DTU_Round_2_Task_data\\4\\Front4.jpg: 480x640 1 2, 1 3, 524.1ms\n",
      "Speed: 4.7ms preprocess, 524.1ms inference, 7.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\saart\\OneDrive\\Desktop\\UAS_DTU_Round_2_Task_data\\4\\Back4.jpg: 480x640 1 3, 427.5ms\n",
      "Speed: 0.0ms preprocess, 427.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "Final Fruit Counts:\n",
      "Red: 0\n",
      "Green: 2\n",
      "Blue: 0\n",
      "Purple: 0\n",
      "Unknown: 0\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_fruits(image_path, model):\n",
    "    \"\"\"Detect fruits and return bounding boxes, centers, and confidence scores\"\"\"\n",
    "    results = model.predict(image_path, conf=0.5)[0]\n",
    "    detections = []\n",
    "    \n",
    "    for box in results.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "        center = (int((x1 + x2)/2), int((y1 + y2)/2))\n",
    "        detections.append({\n",
    "            'bbox': (x1, y1, x2, y2),\n",
    "            'center': center,\n",
    "            'conf': float(box.conf)\n",
    "        })\n",
    "    \n",
    "    return detections\n",
    "\n",
    "def get_color(img, bbox):\n",
    "    \"\"\"Get dominant color of fruit using HSV color space\"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    roi = img[y1:y2, x1:x2]\n",
    "    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Get average HSV values\n",
    "    avg_hsv = np.mean(hsv, axis=(0, 1))\n",
    "    \n",
    "    # More robust color ranges\n",
    "    if (avg_hsv[0] < 15 or avg_hsv[0] > 160) and avg_hsv[1] > 50:\n",
    "        return \"red\"\n",
    "    elif 35 <= avg_hsv[0] < 85 and avg_hsv[1] > 30:\n",
    "        return \"green\"\n",
    "    elif 85 <= avg_hsv[0] < 125 and avg_hsv[1] > 30:\n",
    "        return \"blue\"\n",
    "    elif 125 <= avg_hsv[0] < 160 and avg_hsv[1] > 30:\n",
    "        return \"purple\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def compute_homography(front_img, back_img):\n",
    "    \"\"\"Compute homography matrix between two images\"\"\"\n",
    "    # Use ORB feature detector (faster than SIFT)\n",
    "    orb = cv2.ORB_create(nfeatures=2000)\n",
    "    \n",
    "    # Find keypoints and descriptors\n",
    "    kp1, des1 = orb.detectAndCompute(front_img, None)\n",
    "    kp2, des2 = orb.detectAndCompute(back_img, None)\n",
    "    \n",
    "    if des1 is None or des2 is None:\n",
    "        return None\n",
    "        \n",
    "    # Match features\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(des1, des2)\n",
    "    \n",
    "    # Sort matches by distance\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "    \n",
    "    # Use best matches for homography\n",
    "    good_matches = matches[:min(50, len(matches))]\n",
    "    \n",
    "    src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    \n",
    "    # Compute homography\n",
    "    H, mask = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC, 5.0)\n",
    "    \n",
    "    return H\n",
    "\n",
    "def count_fruits(front_path, back_path, model_path):\n",
    "    \"\"\"Count fruits while avoiding double counting between views\"\"\"\n",
    "    # Initialize\n",
    "    model = YOLO(model_path)\n",
    "    front_img = cv2.imread(front_path)\n",
    "    back_img = cv2.imread(back_path)\n",
    "    color_counts = {\"red\": 0, \"green\": 0, \"blue\": 0, \"purple\": 0, \"unknown\": 0}\n",
    "    \n",
    "    # Get detections\n",
    "    front_detections = detect_fruits(front_path, model)\n",
    "    back_detections = detect_fruits(back_path, model)\n",
    "    \n",
    "    # Compute homography\n",
    "    H = compute_homography(front_img, back_img)\n",
    "    \n",
    "    if H is None:\n",
    "        print(\"Warning: Could not compute homography. Using single view counting.\")\n",
    "        # Fall back to counting only front view\n",
    "        for det in front_detections:\n",
    "            color = get_color(front_img, det['bbox'])\n",
    "            color_counts[color] += 1\n",
    "        return color_counts\n",
    "    \n",
    "    # Transform back centers to front image space\n",
    "    back_centers = np.float32([d['center'] for d in back_detections]).reshape(-1, 1, 2)\n",
    "    transformed_centers = cv2.perspectiveTransform(back_centers, H).reshape(-1, 2)\n",
    "    \n",
    "    # Track matched fruits\n",
    "    matched_back_indices = set()\n",
    "    \n",
    "    # Process front detections first\n",
    "    for front_det in front_detections:\n",
    "        front_color = get_color(front_img, front_det['bbox'])\n",
    "        front_center = np.array(front_det['center'])\n",
    "        \n",
    "        # Look for matches in back view\n",
    "        best_match = None\n",
    "        min_dist = 50  # Pixel threshold for matching\n",
    "        \n",
    "        for i, back_center in enumerate(transformed_centers):\n",
    "            if i in matched_back_indices:\n",
    "                continue\n",
    "                \n",
    "            dist = np.linalg.norm(front_center - back_center)\n",
    "            if dist < min_dist:\n",
    "                back_color = get_color(back_img, back_detections[i]['bbox'])\n",
    "                # Verify match with color and confidence\n",
    "                if back_color == front_color:\n",
    "                    min_dist = dist\n",
    "                    best_match = i\n",
    "        \n",
    "        if best_match is not None:\n",
    "            # Found a match - count only once\n",
    "            matched_back_indices.add(best_match)\n",
    "            color_counts[front_color] += 1\n",
    "        else:\n",
    "            # No match found - count as new fruit\n",
    "            color_counts[front_color] += 1\n",
    "    \n",
    "    # Count unmatched fruits from back view\n",
    "    for i, back_det in enumerate(back_detections):\n",
    "        if i not in matched_back_indices:\n",
    "            color = get_color(back_img, back_det['bbox'])\n",
    "            color_counts[color] += 1\n",
    "    \n",
    "    return color_counts\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    front_path = r'C:\\Users\\saart\\OneDrive\\Desktop\\UAS_DTU_Round_2_Task_data\\4\\Front4.jpg'\n",
    "    back_path = r'C:\\Users\\saart\\OneDrive\\Desktop\\UAS_DTU_Round_2_Task_data\\4\\Back4.jpg'\n",
    "    model_path = r'C:\\Users\\saart\\OneDrive\\Desktop\\best.pt'\n",
    "    \n",
    "    counts = count_fruits(front_path, back_path, model_path)\n",
    "    print(\"\\nFinal Fruit Counts:\")\n",
    "    for color, count in counts.items():\n",
    "        print(f\"{color.capitalize()}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3cc2b4c-2e43-49b6-911f-490413cdced5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\saart\\OneDrive\\Desktop\\UAS_DTU_Round_2_Task_data\\4\\Front4.jpg: 480x640 1 2, 1 3, 564.9ms\n",
      "Speed: 4.0ms preprocess, 564.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\saart\\OneDrive\\Desktop\\UAS_DTU_Round_2_Task_data\\4\\Back4.jpg: 480x640 1 3, 504.8ms\n",
      "Speed: 9.8ms preprocess, 504.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "Fruit Counts:\n",
      "Green: 2\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_fruits(image_path, model):\n",
    "    \"\"\"Detect fruits and return their locations\"\"\"\n",
    "    results = model.predict(image_path)[0]\n",
    "    detections = []\n",
    "    \n",
    "    for box in results.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "        center = (int((x1 + x2)/2), int((y1 + y2)/2))\n",
    "        detections.append({\n",
    "            'bbox': (x1, y1, x2, y2),\n",
    "            'center': center\n",
    "        })\n",
    "    \n",
    "    return detections\n",
    "\n",
    "def get_color(img, bbox):\n",
    "    \"\"\"Get fruit color from bbox region\"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    roi = img[y1:y2, x1:x2]\n",
    "    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "    avg_hsv = np.mean(hsv, axis=(0, 1))\n",
    "    \n",
    "    if avg_hsv[0] < 15 or avg_hsv[0] > 160: return \"red\"\n",
    "    if 35 <= avg_hsv[0] < 85: return \"green\"\n",
    "    if 85 <= avg_hsv[0] < 125: return \"blue\"\n",
    "    if 125 <= avg_hsv[0] < 160: return \"purple\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def match_and_count_fruits(front_path, back_path, model_path):\n",
    "    \"\"\"Count fruits while avoiding double counting\"\"\"\n",
    "    # Initialize\n",
    "    model = YOLO(model_path)\n",
    "    front_img = cv2.imread(front_path)\n",
    "    back_img = cv2.imread(back_path)\n",
    "    color_counts = {\"red\": 0, \"green\": 0, \"blue\": 0, \"purple\": 0, \"unknown\": 0}\n",
    "    \n",
    "    # Get detections\n",
    "    front_dets = detect_fruits(front_path, model)\n",
    "    back_dets = detect_fruits(back_path, model)\n",
    "    \n",
    "    # Get colors for all detections\n",
    "    front_colors = [get_color(front_img, d['bbox']) for d in front_dets]\n",
    "    back_colors = [get_color(back_img, d['bbox']) for d in back_dets]\n",
    "    \n",
    "    # Match fruits between views using color and position\n",
    "    matched_back = set()\n",
    "    \n",
    "    for i, (front_det, front_color) in enumerate(zip(front_dets, front_colors)):\n",
    "        front_center = np.array(front_det['center'])\n",
    "        matched = False\n",
    "        \n",
    "        # Try to find a matching fruit in back view\n",
    "        for j, (back_det, back_color) in enumerate(zip(back_dets, back_colors)):\n",
    "            if j in matched_back:\n",
    "                continue\n",
    "                \n",
    "            if front_color == back_color:\n",
    "                back_center = np.array(back_det['center'])\n",
    "                # Use simple distance threshold for matching\n",
    "                if np.linalg.norm(front_center - back_center) < 100:  # Adjust threshold if needed\n",
    "                    matched_back.add(j)\n",
    "                    matched = True\n",
    "                    break\n",
    "        \n",
    "        # Count the fruit\n",
    "        if matched:\n",
    "            color_counts[front_color] += 1\n",
    "        else:\n",
    "            color_counts[front_color] += 1\n",
    "    \n",
    "    # Count remaining unmatched fruits from back view\n",
    "    for j, back_color in enumerate(back_colors):\n",
    "        if j not in matched_back:\n",
    "            color_counts[back_color] += 1\n",
    "    \n",
    "    return color_counts\n",
    "\n",
    "def main():\n",
    "    # Replace with your paths\n",
    "    front_path = r'C:\\Users\\saart\\OneDrive\\Desktop\\UAS_DTU_Round_2_Task_data\\4\\Front4.jpg'\n",
    "    back_path = r'C:\\Users\\saart\\OneDrive\\Desktop\\UAS_DTU_Round_2_Task_data\\4\\Back4.jpg'\n",
    "    model_path = r'C:\\Users\\saart\\OneDrive\\Desktop\\best.pt'\n",
    "    \n",
    "    counts = match_and_count_fruits(front_path, back_path, model_path)\n",
    "    \n",
    "    print(\"\\nFruit Counts:\")\n",
    "    for color, count in counts.items():\n",
    "        if count > 0:  # Only show colors that were found\n",
    "            print(f\"{color.capitalize()}: {count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b95b211-c1e4-4784-aebc-b52ef842372f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\saart\\OneDrive\\Desktop\\UAS_DTU_Round_2_Task_data\\4\\Front4.jpg: 480x640 1 2, 1 3, 442.5ms\n",
      "Speed: 8.0ms preprocess, 442.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\saart\\OneDrive\\Desktop\\UAS_DTU_Round_2_Task_data\\4\\Back4.jpg: 480x640 1 3, 512.5ms\n",
      "Speed: 0.0ms preprocess, 512.5ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "Final Fruit Counts:\n",
      "Green: 1\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def predict(image_path, model):\n",
    "    \"\"\"Detect fruits using YOLO and return their information\"\"\"\n",
    "    results = model.predict(image_path)[0]\n",
    "    fruits = []\n",
    "    \n",
    "    for box in results.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "        center_x = (x1 + x2) // 2\n",
    "        center_y = (y1 + y2) // 2\n",
    "        \n",
    "        fruits.append({\n",
    "            \"bbox\": (x1, y1, x2, y2),\n",
    "            \"center\": (center_x, center_y)\n",
    "        })\n",
    "    \n",
    "    return fruits\n",
    "\n",
    "def get_color(img, bbox):\n",
    "    \"\"\"Detect color using HSV color space\"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    roi = img[y1:y2, x1:x2]\n",
    "    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "    avg_hue = np.mean(hsv[:, :, 0])\n",
    "    \n",
    "    if 0 <= avg_hue < 15 or 160 <= avg_hue <= 180:\n",
    "        return \"red\"\n",
    "    elif 35 <= avg_hue < 85:\n",
    "        return \"green\"\n",
    "    elif 85 <= avg_hue < 100:\n",
    "        return \"blue\"\n",
    "    elif 100 <= avg_hue < 160:\n",
    "        return \"purple\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def align_images(front_img, back_img):\n",
    "    \"\"\"Align images using feature matching and return homography matrix\"\"\"\n",
    "    orb = cv2.ORB_create(nfeatures=3000)\n",
    "    kp1, des1 = orb.detectAndCompute(front_img, None)\n",
    "    kp2, des2 = orb.detectAndCompute(back_img, None)\n",
    "    \n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(des1, des2)\n",
    "    \n",
    "    # Sort matches by distance\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "    \n",
    "    # Use best matches\n",
    "    good_matches = matches[:min(len(matches), 50)]\n",
    "    \n",
    "    src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    \n",
    "    H, _ = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC, 5.0)\n",
    "    return H\n",
    "\n",
    "def count_fruits(front_path, back_path, model_path):\n",
    "    \"\"\"Main function to count fruits with homography-based matching\"\"\"\n",
    "    # Initialize\n",
    "    model = YOLO(model_path)\n",
    "    front_img = cv2.imread(front_path)\n",
    "    back_img = cv2.imread(back_path)\n",
    "    \n",
    "    # Get detections\n",
    "    front_fruits = predict(front_path, model)\n",
    "    back_fruits = predict(back_path, model)\n",
    "    \n",
    "    # Compute homography matrix\n",
    "    H = align_images(front_img, back_img)\n",
    "    \n",
    "    # Initialize counters\n",
    "    color_counts = {\"red\": 0, \"green\": 0, \"blue\": 0, \"purple\": 0, \"unknown\": 0}\n",
    "    matched_back_indices = set()\n",
    "    \n",
    "    # Process each back fruit\n",
    "    for back_idx, back_fruit in enumerate(back_fruits):\n",
    "        back_x, back_y = back_fruit[\"center\"]\n",
    "        # Transform back coordinates to front view\n",
    "        transformed_point = cv2.perspectiveTransform(\n",
    "            np.array([[[back_x, back_y]]], dtype=np.float32), H\n",
    "        )\n",
    "        tx, ty = transformed_point[0][0]\n",
    "        \n",
    "        # Get color of back fruit\n",
    "        back_color = get_color(back_img, back_fruit[\"bbox\"])\n",
    "        matched = False\n",
    "        \n",
    "        # Compare with front fruits\n",
    "        for front_idx, front_fruit in enumerate(front_fruits):\n",
    "            front_x, front_y = front_fruit[\"center\"]\n",
    "            distance = np.sqrt((front_x - tx)**2 + (front_y - ty)**2)\n",
    "            \n",
    "            if distance < 30:  # Threshold in aligned coordinates\n",
    "                front_color = get_color(front_img, front_fruit[\"bbox\"])\n",
    "                if front_color == back_color:\n",
    "                    matched = True\n",
    "                    matched_back_indices.add(back_idx)\n",
    "                    color_counts[front_color] += 1\n",
    "                    break\n",
    "        \n",
    "        if not matched:\n",
    "            color_counts[back_color] += 1\n",
    "    \n",
    "    # Count unmatched front fruits\n",
    "    for front_fruit in front_fruits:\n",
    "        front_color = get_color(front_img, front_fruit[\"bbox\"])\n",
    "        front_x, front_y = front_fruit[\"center\"]\n",
    "        matched = False\n",
    "        \n",
    "        for back_idx in matched_back_indices:\n",
    "            back_fruit = back_fruits[back_idx]\n",
    "            back_x, back_y = back_fruit[\"center\"]\n",
    "            transformed_point = cv2.perspectiveTransform(\n",
    "                np.array([[[back_x, back_y]]], dtype=np.float32), H\n",
    "            )\n",
    "            tx, ty = transformed_point[0][0]\n",
    "            \n",
    "            distance = np.sqrt((front_x - tx)**2 + (front_y - ty)**2)\n",
    "            if distance < 30:\n",
    "                matched = True\n",
    "                break\n",
    "        \n",
    "        if not matched:\n",
    "            color_counts[front_color] += 1\n",
    "    \n",
    "    return color_counts\n",
    "\n",
    "def main():\n",
    "    # Paths configuration\n",
    "    front_path = r'C:\\Users\\saart\\OneDrive\\Desktop\\UAS_DTU_Round_2_Task_data\\4\\Front4.jpg'\n",
    "    back_path = r'C:\\Users\\saart\\OneDrive\\Desktop\\UAS_DTU_Round_2_Task_data\\4\\Back4.jpg'\n",
    "    model_path = r'C:\\Users\\saart\\OneDrive\\Desktop\\best.pt'\n",
    "    \n",
    "    # Count fruits\n",
    "    counts = count_fruits(front_path, back_path, model_path)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nFinal Fruit Counts:\")\n",
    "    for color, count in counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"{color.capitalize()}: {count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c87cc1f-c3c8-448d-b223-da13b0c4338e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\saart\\OneDrive\\Desktop\\UAS_DTU_Round_2_Task_data\\8\\front.jpg: 480x640 5 2s, 3 3s, 635.1ms\n",
      "Speed: 0.0ms preprocess, 635.1ms inference, 8.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\saart\\OneDrive\\Desktop\\UAS_DTU_Round_2_Task_data\\8\\back.jpg: 480x640 8 2s, 3 3s, 560.6ms\n",
      "Speed: 5.3ms preprocess, 560.6ms inference, 6.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "Final Fruit Counts:\n",
      "Green: 4\n",
      "Blue: 13\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_and_get_colors(img_path, model, img):\n",
    "    \"\"\"Detect fruits and get their colors in one pass\"\"\"\n",
    "    fruits = []\n",
    "    for box in model.predict(img_path)[0].boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "        \n",
    "        # Get color\n",
    "        roi = img[y1:y2, x1:x2]\n",
    "        hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "        hue = np.mean(hsv[:, :, 0])\n",
    "        \n",
    "        # Determine color\n",
    "        if hue < 15 or hue > 160: color = \"red\"\n",
    "        elif 35 <= hue < 85: color = \"green\"\n",
    "        elif 85 <= hue < 100: color = \"blue\"\n",
    "        elif 100 <= hue < 160: color = \"purple\"\n",
    "        else: color = \"unknown\"\n",
    "        \n",
    "        fruits.append({\n",
    "            \"center\": ((x1 + x2) // 2, (y1 + y2) // 2),\n",
    "            \"color\": color\n",
    "        })\n",
    "    return fruits\n",
    "\n",
    "def count_fruits(front_path, back_path, model_path):\n",
    "    # Initialize\n",
    "    model = YOLO(model_path)\n",
    "    front_img = cv2.imread(front_path)\n",
    "    back_img = cv2.imread(back_path)\n",
    "    \n",
    "    # Get detections with colors\n",
    "    front_fruits = detect_and_get_colors(front_path, model, front_img)\n",
    "    back_fruits = detect_and_get_colors(back_path, model, back_img)\n",
    "    \n",
    "    # Compute homography\n",
    "    orb = cv2.ORB_create()\n",
    "    kp1, des1 = orb.detectAndCompute(front_img, None)\n",
    "    kp2, des2 = orb.detectAndCompute(back_img, None)\n",
    "    matches = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True).match(des1, des2)\n",
    "    \n",
    "    good_matches = sorted(matches, key=lambda x: x.distance)[:50]\n",
    "    src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    H, _ = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC, 5.0)\n",
    "    \n",
    "    # Count fruits\n",
    "    counts = {\"red\": 0, \"green\": 0, \"blue\": 0, \"purple\": 0, \"unknown\": 0}\n",
    "    matched_back = set()\n",
    "    \n",
    "    # Transform all back centers at once\n",
    "    back_centers = np.float32([[f[\"center\"]] for f in back_fruits])\n",
    "    if len(back_centers) > 0:\n",
    "        transformed_centers = cv2.perspectiveTransform(back_centers, H).reshape(-1, 2)\n",
    "        \n",
    "        # Match fruits\n",
    "        for front in front_fruits:\n",
    "            front_center = np.array(front[\"center\"])\n",
    "            matched = False\n",
    "            \n",
    "            for i, trans_center in enumerate(transformed_centers):\n",
    "                if i not in matched_back:\n",
    "                    dist = np.linalg.norm(front_center - trans_center)\n",
    "                    if dist < 30 and front[\"color\"] == back_fruits[i][\"color\"]:\n",
    "                        matched_back.add(i)\n",
    "                        matched = True\n",
    "                        break\n",
    "            \n",
    "            if not matched:\n",
    "                counts[front[\"color\"]] += 1\n",
    "    \n",
    "    # Add unmatched back fruits\n",
    "    for i, back in enumerate(back_fruits):\n",
    "        if i not in matched_back:\n",
    "            counts[back[\"color\"]] += 1\n",
    "    \n",
    "    return counts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    counts = count_fruits(\n",
    "        r'C:\\Users\\saart\\OneDrive\\Desktop\\UAS_DTU_Round_2_Task_data\\8\\front.jpg',\n",
    "        r'C:\\Users\\saart\\OneDrive\\Desktop\\UAS_DTU_Round_2_Task_data\\8\\back.jpg',\n",
    "        r'C:\\Users\\saart\\OneDrive\\Desktop\\best.pt'\n",
    "    )\n",
    "    \n",
    "    print(\"\\nFinal Fruit Counts:\")\n",
    "    for color, count in counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"{color.capitalize()}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2410a673-a0c1-4e64-93f4-5c7411e9b217",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
